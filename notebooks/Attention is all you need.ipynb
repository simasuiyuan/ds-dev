{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bed13d-937c-42eb-a54f-8b6e709db6b9",
   "metadata": {},
   "source": [
    "# Attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ad0734-136c-4d4e-9656-6fe0e5a85892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-01 18:51:34] 38532 root {logging-48} INFO - Current process ID: 38532\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import sys\n",
    "from typing import Callable\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append('../')\n",
    "from lib.logger.logging import LoggerInitializer\n",
    "import logging\n",
    "\n",
    "LoggerInitializer().init()\n",
    "logger = logging.getLogger('development')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00889fdc-b4cc-4b7a-8d88-e99c7b700bd2",
   "metadata": {},
   "source": [
    "# Model Archetecture\n",
    "## Encoder-Decoder\n",
    "* the encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time.\n",
    "```\n",
    "    X := x1, x2, x3, x4, ......, xn  (represent featuring of the symbol/word)\n",
    "    Z := z1, z2, z3, z4, ......, zn  (represent order/sequence)\n",
    "         ↓   ↓    ↓   ↓           ↓\n",
    "    Y := y1, y2, y3, y4, ......, yn  (represnt the output gievn zi)\n",
    "```\n",
    "<!-- Image(filename='../references/Attention/overall_structure.jpeg') -->\n",
    "![alt text](../references/Attention/overall_structure.jpeg \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b45be7-5429-4226-9652-79c325c28f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture, base for this and many other model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        src_embed: np.ndarray, \n",
    "        tgt_embed: np.ndarray,\n",
    "        generator: nn.Module\n",
    "    ):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed # x\n",
    "        self.tgt_embed = tgt_embed # z\n",
    "        self.genrator = generator  # y = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"Take in and process: masked src (x) and target sequence\n",
    "        \"\"\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be954ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\" Define standard linear + softmax generation step\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocal) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocal)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e37cd9",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "* N = 6 \n",
    "\n",
    "![alt text](../references/Attention/encoder.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5261b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\" Produce N identical Layers\n",
    "\n",
    "    Args:\n",
    "        module (_type_): _description_\n",
    "        N (_type_): _description_\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d638d",
   "metadata": {},
   "source": [
    "### The encoder is composed of a stack of N = 6 identical layers.\n",
    "### We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a023986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, no_of_features, eps=1e-6) -> None:\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(no_of_features)) \n",
    "        self.b_2 = nn.Parameter(torch.ones(no_of_features)) \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2989609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Core encoder is a stack of N layers\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, N) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N) # N x Layer\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Pass the input and mask through each layer in turn \n",
    "\n",
    "        Args:\n",
    "            x (_type_): _description_\n",
    "            mask (_type_): _description_\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x) # followed by layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b65cc",
   "metadata": {},
   "source": [
    "### We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. \n",
    "\n",
    "### That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.\n",
    "### To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a207579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"a residual connection followed by a layer norm\n",
    "    \n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (_type_): _description_\n",
    "            sublayer (_type_): here should be fully connected feedforward or multi-head self-attention\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x))) # x + : residual connection\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec990d",
   "metadata": {},
   "source": [
    "### Each layer has two sub-layers. \n",
    "        * The first is a multi-head self-attention mechanism, \n",
    "        * and the second is a simple, positionwise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20cf449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2) # 2 sublayer connections\n",
    "         \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x,  mask)) # first sublayer will be self attention multi head model\n",
    "        return self.sublayer[1](x, self.feed_forward) # the second sublayer will be fully connected feed-forward model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a84af",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "* N = 6 \n",
    "\n",
    "![alt text](../references/Attention/decoder.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-dev",
   "language": "python",
   "name": "ds-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
